07  <!-- omit in toc -->
===

**Table of Contents**
- [1. 들어가는 글](#1-들어가는-글)
- [2. 사례연구: VQA](#2-사례연구-vqa)
  - [1. 모델 소개: ViT, GPT-2, DistillBERT](#1-모델-소개-vit-gpt-2-distillbert)
    - [텍스트 프로세서: DistillBERT](#텍스트-프로세서-distillbert)
    - [이미지 프로세서: ViT](#이미지-프로세서-vit)
    - [텍스트 인코더: GPT-2](#텍스트-인코더-gpt-2)
  - [2. 은닉 상태 투영과 융합](#2-은닉-상태-투영과-융합)
  - [3. 크로스-어텐션](#3-크로스-어텐션)
  - [4. 맞춤형 멀티모달 모델](#4-맞춤형-멀티모달-모델)
  - [5. 데이터: Visual QA](#5-데이터-visual-qa)
  - [6.7.](#67)
- [3. 사례 연구: 피드백 기반 강화 학습](#3-사례-연구-피드백-기반-강화-학습)
  - [1. 모델: FLAN-T5](#1-모델-flan-t5)
  - [2. 보상 모델: 감정과 문법 정확도](#2-보상-모델-감정과-문법-정확도)


# 1. 들어가는 글

# 2. 사례연구: VQA
이미지 및 텍스트 처리

## 1. 모델 소개: ViT, GPT-2, DistillBERT

### 텍스트 프로세서: DistillBERT
BERT 모델의 증류 버전으로, 지식 증류(Knowledge Distillation)을 사용하여 더 작고 효율적인 모델로 지식을 전달
높은 정확도로 자연어 텍스트 이해 가능

### 이미지 프로세서: ViT
Imagenet으로 알려진 이미지 데이터셋으로 사전훈련된 모델
사전훈련 때 사용한 이미지 전처리를 재사용하여 여러 장단점을 얻을 수 있음

### 텍스트 인코더: GPT-2

## 2. 은닉 상태 투영과 융합
각 DistilBERT와 ViT모델에서 생성된 출력 텐서의 형식 불일치(차원, 득징 등)를 해결하기 위해 일관되고 관련있는 텍스트 답변을 생성하기 위함

## 3. 크로스-어텐션
멀티모달 시스템이 텍스트와 이미지 입력 사이의 상호작용 및 생성하고자하는 출력 텍스트를 학습할 수 있게 해주는 메커니즘

## 4. 맞춤형 멀티모달 모델
## 5. 데이터: Visual QA
이미지에 대한 개방형 질문과 사람이 주석을 단 답변 쌍

## 6.7.

# 3. 사례 연구: 피드백 기반 강화 학습
피드백 기반 강화 학습(RLF), 인간 피드팩 기반 강화 학습(RLHF), AI 피드백 기반 강화 학습(RLAIF)

1. 언어 모델의 사전 훈련
2. 보상 모델 정의 (잠재적 훈련)
3. 강화 학습으로 언어 모델 파인튜닝

## 1. 모델: FLAN-T5
## 2. 보상 모델: 감정과 문법 정확도
