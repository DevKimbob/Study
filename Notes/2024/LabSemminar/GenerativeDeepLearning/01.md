생성 모델링  <!-- omit in toc -->
===

**Table of Contents**
- [1. 생성 모델링이란](#1-생성-모델링이란)


# 1. 생성 모델링이란
* 훈련
* 샘플링
  > "샘플링은 훈련 데이터를 통해 훈련한 "생성 모델"을 통해 원본 데이터셋에 없는 새롭고 사실적인 말 이미지를 만들 수 있다."
  > -> 샘플링은 생성 모델을 사용하는 행위를 뜻하는 말?
* 훈련 데이터
* 샘플
* 특성

생성 모델은 결정적이 아니고 확률적이어야 함.

## 1. 생성 모델링과 판별 모델링
판별 모델링 예시 : 해당 그림이 반 고흐의 그림일 확률을 출력
-> 각 훈련 데이터에 레이블이 존재해야 함

생성 모델링 예시 : 예쁜 그림을 출력
-> 훈련 데이터에 레이블이 빌요하지 않음

* 판별 모델링 : p(y|x) 를 추정
  * 샘플 x가 주어졌을 때 레이블 y의 확률을 모델링
* 생성 모델링 : p(x) 를 추정
  * 샘플 x를 관측할 확률을 모델링

## 3. 생성 모델링과 AI
1. 단순히 데이터를 분류하지 않고 데이터 분포를 완전히 이해하도록 관심
2. 강화 학습 같은 타 AI 분야를 주도
3. 인간과 비슷한 지능의 머신을 만들 경우 생성 모델이 그 일부가 되어야 함

# 2. 첫 번째 생성 모델
## 1. 간단한 생성 모델
1. 2차원 좌표평면에서 특정한 규칙을 통해 포인트 집합 X를 생성한다고 가정
2. 위 규칙이 p_data이며, 포인트 생성 모델 p_model을 만드는 것이 최종 목표
3. 이 때 p_model은 p_data의 추정이며, p_model 분포로부터 샘플링을 할 경우 p_data 규칙에 따른 포인트를 생성했다고 볼 수 있음

## 3. 표현 학습
고차원 표본 공간을 직접 모델링하는 방식이 아니라 대신 저차원의 잠재 공간을 사용해 훈련 세트의 각 샘플을 표현하고 이를 원본 공간의 포인트에 매핑
잠재 공간의 한 포인트를 고차원의 공간에 매핑하는 매핑 함수를 학습해야 함
> 인코더-디코더 기법도 표현 학습의 일종이라고 볼 수 있을 듯
> 고차원의 매니폴드를 샘플링 가능한 단순한 잠재 공간으로 변환

# 3. 핵심 확률 이론
* 표본 공간
  * 샘플 x가 가질 수 있는 모든 값의 집합
* 확률 밀도 함수(밀도 함수)
  * 포인트 x를 0과 1 사이의 숫자로 매핑하는 함수 p(x)
  * 표본 공간에 있는 모든 포인트에 대해 밀도 함수를 적분했을 때 1이 되어야 잘 정의된 확률분포
  * 실제 밀도 함수 p_data(x)는 하나이지만, p_data(x)를 추정하는 데 사용할 수 있는 밀도 함수 p_model(x)는 무수히 많음
* 모수 모델링(parametric modeling)
  * 안정적인 p_model(x)를 찾는 데 사용할 수 있는 기법
  * 모수 모델 : 유한한 개수의 파라미터 theta를 사용해 기술할 수 있는 밀도 함수 p_theta(x)의 한 종류
* 가능도
  * 파라미터 집합 theta의 가능도 L(theta|x)는 관측된 포인트 x가 주어졌을 때 theta의 타당성을 측정하는 함수
    * L(theta|x) = p_theta(x)
  * 즉, 관측 포인트 x가 주어졌을 때 theta의 가능도는 포인트 x에서 theta를 파라미터로 가진 밀도 함수의 값으로 정의
    * L(theta|X) = MUL(p_theta(x)) where x is in X
  * 계산 비용 감소를 위해 로그 가능도를 사용하는 경우가 많음
    * l(theta|X) = SUM(log p_theta(x)) where x is in X
  * 실제 데이터 생성 분포가 theta를 파라미터로 가진 모델이라면, 파라미터 집합 theta의 가능도를 데이터가 발견될 확률이라고 정의
  * 가능도는 데이터가 아니라 파라미터의 함수로 주어진 파라미터 집합이 올바른지에 대한 확률로 해석해선 안됨. 파라미터 공간의 확률 분포가 아니며, 파라미터에 대해 적분합할 경우 1이 되지 않음
* 최대 가능도 추정
  * theta_hat 을 추정하는 기법
    * 관측된 데이터 X를 가장 잘 설명하는 밀도 함수 p_theta(x)의 파라미터 집합 theta
    * theta_hat = argmax_theta l (theta|X)
  * 신경망은 일반적으로 손실 함수를 최소화
  * 음의 로그 가능도를 최소화하는 파라미터 집합을 찾는 것과 같음
    * theta_hat = argmax_theta (- l (theta|X)) = argmin_theta (- log p_theta(X))